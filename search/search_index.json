{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick Start","text":""},{"location":"#getting-started-with-deepstream-101","title":"Getting Started with deepstream-101","text":"<p>New developers often find it challenging to set up the environment and run DeepStream Python applications. This page is intended for beginners seeking a deep understanding of the <code>NVIDIA DeepStream SDK</code>. If you are a developer who wants contribute to the project, please visit the development page.</p>"},{"location":"#installation","title":"Installation","text":"<p>Visit this guide</p>"},{"location":"#sample-applications","title":"Sample Applications","text":"<p>The sample applications included in this repository demonstrate how to build and manage DeepStream pipelines using Python.</p> <p>For detailed examples, please refer to the DeepStream Python Apps.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>Feel free to create a bug report or a feature request on our repo</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#supported-platforms","title":"Supported Platforms","text":"<p>This release supports Ubuntu 20.04 for DeepStream SDK 6.3 with Python 3.8. Other versions may require adjustments. The recommended setup is Docker-based for both dGPUs and Jetson platforms, though a base environment can also be used.</p>"},{"location":"install/#installation","title":"Installation","text":"<ol> <li> <p>Pull the DeepStream Docker container</p> <p>Run the following command to download the official NVIDIA DeepStream container:   </p><pre><code>docker pull nvcr.io/nvidia/deepstream:6.3-triton-multiarch\n</code></pre> </li> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/quangdungluong/deepstream-101\ncd deepstream-101\n</code></pre> </li> <li> <p>Run the docker container     Start a DeepStream container with necessary permissions and volume mounting     </p><pre><code>docker run \\\n    -it --rm \\\n    --runtime nvidia \\\n    --device /dev/snd \\\n    -v ./:/deepstream \\\n    -w /deepstream \\\n    --name deepstream-101 \\\n    -p 8554:8554 \\\n    nvcr.io/nvidia/deepstream:6.3-triton-multiarch\n</code></pre> </li> <li> <p>Install necessary libraries</p> <p>Inside the container, install the Python bindings for DeepStream   </p><pre><code># Download and install pyds\nwget https://github.com/NVIDIA-AI-IOT/deepstream_python_apps/releases/download/v1.1.8/pyds-1.1.8-py3-none-linux_x86_64.whl\npip3 install pyds-1.1.8-py3-none-linux_x86_64.whl\n# Install cuda-python\npip3 install cuda-python\n</code></pre> </li> <li> <p>Setup complete - Start exploring DeepStream in Python</p> <p>You're now ready to dive into DeepStream Python applications.\ud83d\ude80   Let me know if you need any modifications.</p> </li> </ol>"},{"location":"python_binding/","title":"Python binding","text":""},{"location":"python_binding/#build-and-install-gst-python","title":"Build and install gst-python","text":"<pre><code>apt-get update\napt install -y python3-gi python3-dev python3-gst-1.0 python-gi-dev git python-dev \\\n    python3 python3-pip python3.8-dev cmake g++ build-essential libglib2.0-dev \\\n    libglib2.0-dev-bin python-gi-dev libtool m4 autoconf automake\n\napt-get install -y apt-transport-https ca-certificates -y\nupdate-ca-certificates\n\napt install libgirepository1.0-dev \n</code></pre> <pre><code>cd 3rdparty/gst-python/\n./autogen.sh\nmake\nmake install\n</code></pre> <pre><code>cd /apps/bindings\n\nmkdir build\ncd build\ncmake ..\nmake -j$(nproc)\n</code></pre>"},{"location":"apps/deepstream-rt-src-add-del/","title":"Deepstream rt src add del","text":"<pre><code>python3 deepstream_rt_src_add_del.py file:///opt/nvidia/deepstream/deepstream-6.3/samples/streams/sample_1080p_h264.mp4\n</code></pre>"},{"location":"apps/deepstream-rtsp-in-rtsp-out/","title":"deepstream-rtsp-in-rtsp-out","text":""},{"location":"apps/deepstream-rtsp-in-rtsp-out/#deepstream-rtsp-in-rtsp-out","title":"Deepstream rtsp in rtsp out","text":"<p>This tutorial demonstrates how to build a Deepstream application that:</p> <ul> <li>Accepts an RTSP stream as input and outputs the inference results as an RTSP stream</li> <li>Supports both <code>nvinfer</code> and <code>nvinferserver</code> as inference engines, allowing flexibility in deployment</li> </ul>"},{"location":"apps/deepstream-rtsp-in-rtsp-out/#nvinferserver-notes","title":"<code>nvinferserver</code> notes","text":"<p>If <code>nvinferserver</code> is selected, the application:</p> <ul> <li>Uses an SSD-based neural network running on Triton Inference Server</li> <li>Configures Triton to apply custom post-processing</li> <li>Extracts inference results and converts them into bounding boxes</li> <li>Applies Non-Maximum Suppression (NMS) to refine detection outputs</li> <li>Stores detected objects in the metadata for further processing</li> </ul>"},{"location":"apps/deepstream-rtsp-in-rtsp-out/#download-model","title":"Download model","text":"<pre><code>export MODEL_REPO_DIR=/apps/models\nmkdir -p ${MODEL_REPO_DIR}/ssd_inception_v2_coco_2018_01_28/1\nwget -O /tmp/ssd_inception_v2_coco_2018_01_28.tar.gz \\\n     http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz\ncd /tmp &amp;&amp; tar xzf ssd_inception_v2_coco_2018_01_28.tar.gz\nmv /tmp/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb \\\n    ${MODEL_REPO_DIR}/ssd_inception_v2_coco_2018_01_28/1/model.graphdef\nrm -fr /tmp/ssd_inception_v2_coco_2018_01_28.tar.gz  /tmp/ssd_inception_v2_coco_2018_01_28\n</code></pre>"},{"location":"apps/deepstream-rtsp-in-rtsp-out/#running-the-application","title":"Running the application","text":"<pre><code>cd apps/deepstream-rtsp-in-rtsp-out\n\n# nvinfer\npython3 deepstream_test1_rtsp_in_rtsp_out.py -i file:///opt/nvidia/deepstream/deepstream-6.3/samples/streams/sample_1080p_h264.mp4\n\n# or nvinferserver\npython3 deepstream_test1_rtsp_in_rtsp_out.py -i file:///opt/nvidia/deepstream/deepstream-6.3/samples/streams/sample_1080p_h264.mp4 -g nvinferserver\n</code></pre>"},{"location":"apps/deepstream-smart-record/","title":"Deepstream smart record","text":"<pre><code>python3 deepstream_smart_record.py rtsp://akacamai:fpt12345@103.176.147.25:8554/camera_146\n</code></pre>"},{"location":"apps/deepstream-test1/","title":"deepstream-test1","text":""},{"location":"apps/deepstream-test1/#deepstream-test-1","title":"Deepstream Test 1","text":"<p>The <code>deepstream-test1</code> application shows how to use NVIDIA DeepStream SDK elements in a pipeline and extract meaningful insights from a video stream. This sample creates an instance of the <code>nvinfer</code> element, which uses the TensorRT API to perform inference on a model. It is crucial to configure the <code>nvinfer</code> element correctly, as many of its behaviors are parameterized through configuration files.</p>"},{"location":"apps/deepstream-test1/#overview","title":"Overview","text":"<p>The <code>deepstream-test1</code> follows this pipeline:</p> <ul> <li>File Source (<code>filesrc</code>): reads a video file</li> <li>H.264 Parser (<code>h264parse</code>): parses the H.264 encoded stream, ensures the input stream is properly formatted before decoding</li> <li>Decoder (<code>nvv4l2decoder</code>): decodes the stream into raw video frames using NVIDIA hardware acceleration</li> <li>Stream Muxer (<code>nvvstreammux</code>): batches frames for efficient processing/combines multiple input streams into a single batched buffer for processing</li> <li>Inference (<code>nvinfer</code>): runs a deep learning model on the batched frames</li> <li>On-Screen Display (<code>nvdsosd</code>): draws bounding boxes and labels on detected objects</li> <li>Fake Sink (<code>fakesink</code>): discards the output without rendering. This is especially useful when running on remote server or Jetson devices without a monitor</li> </ul> <p></p>"},{"location":"apps/deepstream-test1/#preparing-the-model","title":"Preparing the model","text":"<p>This sample uses the TrafficCamNet model, which detects 4 classes: <code>Vehicle</code>, <code>Road Sign</code>, <code>Two-Wheeler</code> and <code>Person</code>. The configuration file used for this detector is <code>dstest1_pgie_config.txt</code>.</p>"},{"location":"apps/deepstream-test1/#downloading-the-model","title":"Downloading the model","text":"<p>To use the model, download it from NGC Models using the following commands: </p><pre><code>wget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/trafficcamnet/pruned_onnx_v1.0.3/files?redirect=true&amp;path=resnet18_trafficcamnet_pruned.onnx' -O models/Primary_Detector/resnet18_trafficcamnet_pruned.onnx\nwget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/trafficcamnet/pruned_onnx_v1.0.3/files?redirect=true&amp;path=resnet18_trafficcamnet_pruned_int8.txt' -O models/Primary_Detector/resnet18_trafficcamnet_pruned_int8.txt\n</code></pre>"},{"location":"apps/deepstream-test1/#running-the-application","title":"Running the application","text":"<p>The application will automatically export the model into an INT8 engine before execution. </p><pre><code>cd apps/deepstream-test1\n# Run the test application with an H.264 elementary stream\npython3 deepstream_test_1.py /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.h264\n</code></pre>"},{"location":"apps/deepstream-test2/","title":"deepstream-test2","text":""},{"location":"apps/deepstream-test2/#deepstream-test-2","title":"Deepstream Test 2","text":"<p>This guide explains how to set up and run the <code>deepstream-test2</code> application, which demonstrates object detection and tracking using NVIDIA DeepStream. It leverages <code>nvinfer</code> as the primary inference engine (pgie) and <code>nvtracker</code> for object tracking. Additionally, two secondary inference engines (sgies) classify detected objects.</p>"},{"location":"apps/deepstream-test2/#overview","title":"Overview","text":""},{"location":"apps/deepstream-test2/#preparing-the-model","title":"Preparing the model","text":"<p>This sample application uses the following pre-trained models from NVIDIA\u2019s NGC catalog:</p> <ul> <li>TrafficCamNet model, which detects 4 classes: <code>Vehicle</code>, <code>Road Sign</code>, <code>Two-Wheeler</code> and <code>Person</code>. The configuration file used for this detector is <code>dstest2_pgie_config.txt</code>.</li> </ul> <ul> <li>VehicleMakeNet model, which classifies car makes in images. The configuration file uses for this classifier is <code>dstest2_sgie1_config.txt</code>.</li> </ul> <ul> <li>VehicleTypeNet model, which classifies vehicle types. The configuration file uses for this classifier is <code>dstest2_sgie2_config.txt</code>.</li> </ul>"},{"location":"apps/deepstream-test2/#downloading-the-model","title":"Downloading the model","text":"<p>Before running the application, you need to download the required models.</p>"},{"location":"apps/deepstream-test2/#download-trafficcam-model-label","title":"Download TrafficCam model &amp; label","text":"<p>Follow the instruction in deepstream-test1</p>"},{"location":"apps/deepstream-test2/#download-vehiclemake-model-label","title":"Download VehicleMake model &amp; label","text":"<p>To use the model, download it from NGC Models using the following commands: </p><pre><code>wget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/vehiclemakenet/pruned_onnx_v1.1.0/files?redirect=true&amp;path=resnet18_pruned.onnx' -O models/Secondary_VehicleMake/resnet18_vehiclemakenet_pruned.onnx\n\nwget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/vehiclemakenet/pruned_onnx_v1.1.0/files?redirect=true&amp;path=labels.txt' -O models/Secondary_VehicleMake/labels.txt\n</code></pre>"},{"location":"apps/deepstream-test2/#download-vehicletype-model-label","title":"Download VehicleType model &amp; label","text":"<p>To use the model, download it from NGC Models using the following commands: </p><pre><code>wget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/vehicletypenet/pruned_onnx_v1.1.0/files?redirect=true&amp;path=resnet18_pruned.onnx' -O models/Secondary_VehicleTypes/resnet18_vehicletypenet_pruned.onnx\n\nwget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/vehicletypenet/pruned_onnx_v1.1.0/files?redirect=true&amp;path=labels.txt' -O models/Secondary_VehicleTypes/labels.txt\n</code></pre>"},{"location":"apps/deepstream-test2/#running-the-application","title":"Running the application","text":"<p>The application will automatically export the model into an INT8 engine before execution. </p><pre><code>cd deepstream-test2\n# Run the test application with an H.264 elementary stream\npython3 deepstream_test_2.py /opt/nvidia/deepstream/deepstream-6.3/samples/streams/sample_720p.h264\n</code></pre> <p>This guide ensures a smooth setup and execution of the DeepStream Test 2 application. \ud83d\ude80</p>"},{"location":"apps/deepstream-test3/","title":"deepstream-test3","text":""},{"location":"apps/deepstream-test3/#deepstream-test-3","title":"Deepstream Test 3","text":"<p>This tutorial provides a step-by-step guide to building a DeepStream pipeline that:</p> <ul> <li>Utilizes <code>uridecodebin</code> for handling different types of inputs (RTSP streams, files, different codecs).</li> <li>Demonstrates how to enable latency measurement using a probe function.</li> <li>Showcases the use of <code>nvinferserver</code>, including <code>nvinferserver</code> with gRPC-based inference.</li> </ul>"},{"location":"apps/deepstream-test3/#overview","title":"Overview","text":""},{"location":"apps/deepstream-test3/#preparing-the-model","title":"Preparing the model","text":"<p>This sample application uses the PeopleNet models from NVIDIA's NGC catalog.</p>"},{"location":"apps/deepstream-test3/#downloading-the-model","title":"Downloading the model","text":"<p>To use the model, download it from NGC Models using the following commands: </p><pre><code>wget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/peoplenet/deployable_quantized_v2.6/files?redirect=true&amp;path=resnet34_peoplenet_int8.etlt' -O models/tao_pretrained_models/peopleNet/resnet34_peoplenet_int8.etlt\n\nwget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/peoplenet/deployable_quantized_v2.6/files?redirect=true&amp;path=resnet34_peoplenet_int8.txt' -O models/tao_pretrained_models/peopleNet/resnet34_peoplenet_int8.txt\n</code></pre>"},{"location":"apps/deepstream-test3/#running-the-application","title":"Running the application","text":"<p>The application will automatically export the model into an INT8 engine before execution. </p><pre><code>cd apps/deepstream_test_3.py\n\npython3 deepstream_test_3.py -i file:///opt/nvidia/deepstream/deepstream-6.3/samples/streams/sample_1080p_h264.mp4 \\\n        -c config_infer_primary_peoplenet.txt \\\n        -g nvinfer \\\n        --no-display\n\npython3 deepstream_test_3.py -i file:///opt/nvidia/deepstream/deepstream-6.3/samples/streams/sample_1080p_h264.mp4 \\\n        -c config_triton_infer_primary_peoplenet.txt \\\n        -g nvinferserver \\\n        --no-display\n\npython3 deepstream_test_3.py -i file:///opt/nvidia/deepstream/deepstream-6.3/samples/streams/sample_1080p_h264.mp4 \\\n        -c config_triton_infer_primary_peoplenet.txt \\\n        -g nvinferserver-grpc \\\n        --no-display\n</code></pre>"}]}